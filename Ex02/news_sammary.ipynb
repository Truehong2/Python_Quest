{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsL8ftsI/D7oYw2iK1061d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiseijuuzzz/Python_Quest/blob/main/news_sammary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, Attention\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 필요한 NLTK 자원 다운로드\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 데이터셋 다운로드\n",
        "url = \"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\"\n",
        "file_path = \"news_summary_more.csv\"\n",
        "urllib.request.urlretrieve(url, filename=file_path)\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "data = pd.read_csv(file_path, encoding='iso-8859-1')\n",
        "\n",
        "# 전처리 함수 정의\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # 소문자 변환\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # 특수문자 제거\n",
        "    stop_words = set(stopwords.words('english'))  # 영어 불용어 집합 가져오기\n",
        "    word_tokens = word_tokenize(text)  # 단어 토큰화\n",
        "    text = ' '.join([w for w in word_tokens if not w in stop_words])  # 불용어 제거\n",
        "    return text\n",
        "\n",
        "# 전처리 적용\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "data['headlines'] = data['headlines'].apply(preprocess_text)\n",
        "\n",
        "# 입력과 출력 데이터 분리\n",
        "X = data['text']\n",
        "Y = data['headlines']\n",
        "\n",
        "# 학습과 테스트 데이터 분리\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 토크나이저 정의 및 텍스트 데이터에 적용\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(X_train))\n",
        "\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(Y_train))\n",
        "\n",
        "# 디코더 입력에 'starttoken' 추가\n",
        "y_tokenizer.word_index['starttoken'] = len(y_tokenizer.word_index) + 1\n",
        "y_tokenizer.index_word[len(y_tokenizer.word_index)] = 'starttoken'\n",
        "\n",
        "# 정수 인코딩과 패딩 for Encoder Inputs\n",
        "X_train_seq = x_tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = x_tokenizer.texts_to_sequences(X_test)\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=30, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=30, padding='post')\n",
        "\n",
        "# 정수 인코딩과 패딩 for Decoder Inputs and Outputs (Start token and padding)\n",
        "Y_train_seq = y_tokenizer.texts_to_sequences(Y_train)\n",
        "Y_test_seq = y_tokenizer.texts_to_sequences(Y_test)\n",
        "\n",
        "Y_train_input = pad_sequences(Y_train_seq, maxlen=8, padding='post')\n",
        "Y_test_input = pad_sequences(Y_test_seq, maxlen=8, padding='post')\n",
        "\n",
        "# 어텐션 메커니즘을 사용한 seq2seq 모델 설계\n",
        "latent_dim = 300\n",
        "\n",
        "encoder_inputs = Input(shape=(30,))\n",
        "enc_emb = Embedding(len(x_tokenizer.word_index) + 1, latent_dim, trainable=True)(encoder_inputs)\n",
        "\n",
        "# Encoder LSTM\n",
        "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_inputs = Input(shape=(8,))\n",
        "dec_emb_layer = Embedding(len(y_tokenizer.word_index) + 1, latent_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# 어텐션 메커니즘 추가\n",
        "attn_layer = Attention()\n",
        "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# 어텐션 출력과 디코더 LSTM 출력 연결\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# Dense 레이어\n",
        "decoder_dense = Dense(len(y_tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# 어텐션 메커니즘을 사용한 seq2seq 모델 학습\n",
        "history = model.fit([X_train_pad, Y_train_input[:, :-1]], Y_train_input[:, 1:],\n",
        "                    epochs=50, batch_size=128, validation_data=([X_test_pad, Y_test_input[:, :-1]], Y_test_input[:, 1:]))\n",
        "\n",
        "# 학습 과정 시각화\n",
        "plt.plot(history.history['loss'], label='훈련 손실')\n",
        "plt.plot(history.history['val_loss'], label='검증 손실')\n",
        "plt.xlabel('에포크')\n",
        "plt.ylabel('손실')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 학습된 모델을 사용하여 추상적 요약 생성\n",
        "generated_summaries = []\n",
        "for i in range(len(X_test)):\n",
        "    text = X_test.iloc[i]\n",
        "    text = \"starttoken \" + text + \" endtoken\"\n",
        "    text_seq = x_tokenizer.texts_to_sequences([text])\n",
        "    text_pad = pad_sequences(text_seq, maxlen=30, padding='post')\n",
        "\n",
        "    pred_seq = model.predict([text_pad, Y_test_input[i].reshape(1, -1)[:, :-1]])\n",
        "    pred_seq = pred_seq.argmax(axis=-1)\n",
        "\n",
        "    pred_summary = ' '.join([y_tokenizer.index_word[idx] for idx in pred_seq[0]])\n",
        "    pred_summary = pred_summary.replace('starttoken', '').replace('endtoken', '').strip()\n",
        "    generated_summaries.append(pred_summary)\n",
        "\n",
        "# 실제 결과와 요약문 비교\n",
        "for i in range(5):  # 상위 5개 샘플에 대해서만 확인\n",
        "    print(\"원래 요약문:\", Y_test.iloc[i])\n",
        "    print(\"추상적 요약 결과:\", generated_summaries[i])\n",
        "    print()\n",
        "\n",
        "# 추출적 요약 결과 확인\n",
        "for i in range(5):  # 상위 5개 샘플에 대해서만 확인\n",
        "    text = X_test.iloc[i]\n",
        "    extracted_summary = ' '.join(sent_tokenize(text)[:2])\n",
        "    print(\"원문:\", text)\n",
        "    print(\"추출적 요약 결과:\", extracted_summary)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic0FKN8eEegi",
        "outputId": "bbc0bd57-7246-4dcc-8c3b-5ad9e64ed71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}